{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":71485,"databundleVersionId":8059942,"sourceType":"competition"},{"sourceId":8029842,"sourceType":"datasetVersion","datasetId":4732809},{"sourceId":8141507,"sourceType":"datasetVersion","datasetId":4813598},{"sourceId":8166166,"sourceType":"datasetVersion","datasetId":4832208},{"sourceId":8339744,"sourceType":"datasetVersion","datasetId":4791897}],"dockerImageVersionId":30699,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#os\nimport copy\nfrom glob import glob\nimport re\nimport random\nimport warnings\nimport gc\nfrom collections import Counter\nimport spacy\nimport pickle\n\n#string\nimport string\n\n#data\nfrom datasets import Dataset\nfrom pathlib import Path\n\n#math\nimport numpy as np \nimport pandas as pd\nimport polars as pl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n#nlp\nimport nltk\nfrom nltk import PorterStemmer, WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\n\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\nfrom transformers import Trainer, TrainingArguments,DataCollatorWithPadding\n\n#model\nimport torch\nfrom scipy.special import softmax\nfrom lightgbm import log_evaluation, early_stopping\nimport lightgbm as lgb\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier \nfrom sklearn.ensemble import AdaBoostClassifier,GradientBoostingClassifier,BaggingClassifier\nfrom sklearn.linear_model import LogisticRegression, Perceptron\nfrom sklearn.svm import LinearSVC\nfrom sklearn.naive_bayes import GaussianNB,MultinomialNB,ComplementNB\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\nfrom sklearn.pipeline import Pipeline\nfrom imblearn.ensemble import BalancedBaggingClassifier\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.datasets import make_classification\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, f1_score\nfrom sklearn.metrics import cohen_kappa_score","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:58.968966Z","iopub.status.idle":"2024-05-06T19:43:58.969350Z","shell.execute_reply.started":"2024-05-06T19:43:58.969169Z","shell.execute_reply":"2024-05-06T19:43:58.969185Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Цель этого конкурса - обучить модель для оценки студенческих эссе. Ваши усилия необходимы для сокращения больших затрат и времени на ручное оценивание эссе. Надежные автоматизированные методы позволят ввести эссе в тестирование - ключевой показатель успеваемости студентов, который в настоящее время часто не используется из-за сложностей с выставлением оценок.\n\nВ этом конкурсе вы будете работать с крупнейшим набором данных по сочинениям, находящимся в открытом доступе и соответствующим современным стандартам оценки знаний учащихся. Сможете ли вы помочь создать алгоритм оценки сочинений с открытым исходным кодом, который улучшит результаты конкурса Automated Student Assessment Prize (ASAP), проведенного в 2012 году?\n\nКвадратичная взвешенная каппа рассчитывается следующим образом. Сначала строится матрица гистограмм N x N O, в которой Oi,j соответствует количеству эссе-идов i (фактических), получивших предсказанное значение j. Матрица весов N-by-N, w, рассчитывается на основе разницы между фактическими и предсказанными значениями:\n\nРассчитывается N-by-N гистограммная матрица ожидаемых исходов, E, в предположении, что корреляция между значениями отсутствует. Она рассчитывается как внешнее произведение фактического гистограммного вектора исходов и предсказанного гистограммного вектора, нормированного таким образом, чтобы E и O имели одинаковую сумму.\n\nИз этих трех матриц вычисляется квадратичная взвешенная каппа: ","metadata":{}},{"cell_type":"code","source":"warnings.filterwarnings('ignore')\nnltk.download('wordnet')","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:58.970417Z","iopub.status.idle":"2024-05-06T19:43:58.970779Z","shell.execute_reply.started":"2024-05-06T19:43:58.970585Z","shell.execute_reply":"2024-05-06T19:43:58.970599Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"MAX_LENGTH = 1024\nTEST_DATA_PATH = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/test.csv\"\nMODEL_PATH = '/kaggle/input/aes2-400-20240419134941/*/*'\nEVAL_BATCH_SIZE = 1","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:58.972110Z","iopub.status.idle":"2024-05-06T19:43:58.972602Z","shell.execute_reply.started":"2024-05-06T19:43:58.972354Z","shell.execute_reply":"2024-05-06T19:43:58.972374Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"models = glob(MODEL_PATH)\ntokenizer = AutoTokenizer.from_pretrained(models[0])","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:58.973858Z","iopub.status.idle":"2024-05-06T19:43:58.974326Z","shell.execute_reply.started":"2024-05-06T19:43:58.974084Z","shell.execute_reply":"2024-05-06T19:43:58.974104Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def tokenize(sample):\n    return tokenizer(sample['full_text'], max_length=MAX_LENGTH, truncation=True)\n\ndf_test = pd.read_csv(TEST_DATA_PATH)\nds = Dataset.from_pandas(df_test).map(tokenize).remove_columns(['essay_id', 'full_text'])\n\nargs = TrainingArguments(\".\", per_device_eval_batch_size=EVAL_BATCH_SIZE, report_to=\"none\")\n\npredictions = []\nfor model in models:\n    model = AutoModelForSequenceClassification.from_pretrained(model)\n    trainer = Trainer(model=model, \n        args=args, \n        data_collator=DataCollatorWithPadding(tokenizer), \n        tokenizer=tokenizer\n    )    \n    preds = trainer.predict(ds).predictions\n    predictions.append(softmax(preds, axis=-1))\n    del model, trainer\n    torch.cuda.empty_cache()\n    gc.collect()","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:58.975420Z","iopub.status.idle":"2024-05-06T19:43:58.975960Z","shell.execute_reply.started":"2024-05-06T19:43:58.975654Z","shell.execute_reply":"2024-05-06T19:43:58.975697Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predicted_score = 0.\nfor p in predictions:\n    predicted_score += p\n    \npredicted_score /= len(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:58.977881Z","iopub.status.idle":"2024-05-06T19:43:58.978233Z","shell.execute_reply.started":"2024-05-06T19:43:58.978065Z","shell.execute_reply":"2024-05-06T19:43:58.978079Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test['score'] = predicted_score.argmax(-1) + 1\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:58.979837Z","iopub.status.idle":"2024-05-06T19:43:58.980192Z","shell.execute_reply.started":"2024-05-06T19:43:58.980014Z","shell.execute_reply":"2024-05-06T19:43:58.980028Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test[['essay_id', 'score']].to_csv('submission1.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:58.981323Z","iopub.status.idle":"2024-05-06T19:43:58.981635Z","shell.execute_reply.started":"2024-05-06T19:43:58.981478Z","shell.execute_reply":"2024-05-06T19:43:58.981490Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"predicted_score = 0.\nfor p in predictions:\n    predicted_score += p\n    \npredicted_score /= len(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:58.982991Z","iopub.status.idle":"2024-05-06T19:43:58.983347Z","shell.execute_reply.started":"2024-05-06T19:43:58.983171Z","shell.execute_reply":"2024-05-06T19:43:58.983186Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test['score'] = predicted_score.argmax(-1) + 1\ndf_test.head()","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:58.984699Z","iopub.status.idle":"2024-05-06T19:43:58.985066Z","shell.execute_reply.started":"2024-05-06T19:43:58.984868Z","shell.execute_reply":"2024-05-06T19:43:58.984882Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df_test[['essay_id', 'score']].to_csv('submission1.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:58.986027Z","iopub.status.idle":"2024-05-06T19:43:58.986338Z","shell.execute_reply.started":"2024-05-06T19:43:58.986180Z","shell.execute_reply":"2024-05-06T19:43:58.986193Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Загрузка данных ","metadata":{}},{"cell_type":"code","source":"columns = [  \n    (\n        pl.col(\"full_text\").str.split(by=\"\\n\\n\").alias(\"paragraph\")\n    ),\n]\nPATH = \"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/\"\n\ntrain = pl.read_csv(PATH + \"train.csv\").with_columns(columns)\ntest = pl.read_csv(PATH + \"test.csv\").with_columns(columns)\n\ntrain.head(1)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:58.987575Z","iopub.status.idle":"2024-05-06T19:43:58.988097Z","shell.execute_reply.started":"2024-05-06T19:43:58.987838Z","shell.execute_reply":"2024-05-06T19:43:58.987859Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"nlp = spacy.load(\"en_core_web_sm\")\n\nwith open('/kaggle/input/english-word-hx/words.txt', 'r') as file:\n    english_vocab = set(word.strip().lower() for word in file)\ndef count_spelling_errors(text):\n    doc = nlp(text)\n    lemmatized_tokens = [token.lemma_.lower() for token in doc]\n    spelling_errors = sum(1 for token in lemmatized_tokens if token not in english_vocab)\n    return spelling_errors","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:58.989867Z","iopub.status.idle":"2024-05-06T19:43:58.990190Z","shell.execute_reply.started":"2024-05-06T19:43:58.990032Z","shell.execute_reply":"2024-05-06T19:43:58.990045Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Препроцессинг","metadata":{}},{"cell_type":"code","source":"cList = {\n  \"ain't\": \"am not\",\"aren't\": \"are not\",\"can't\": \"cannot\",\"can't've\": \"cannot have\",\"'cause\": \"because\",  \"could've\": \"could have\",\"couldn't\": \"could not\",\"couldn't've\": \"could not have\",\"didn't\": \"did not\",\"doesn't\": \"does not\",\"don't\": \"do not\",\"hadn't\": \"had not\",\"hadn't've\": \"had not have\",\"hasn't\": \"has not\",\n  \"haven't\": \"have not\",\"he'd\": \"he would\",\"he'd've\": \"he would have\",\"he'll\": \"he will\",\"he'll've\": \"he will have\",\"he's\": \"he is\",\n  \"how'd\": \"how did\",\"how'd'y\": \"how do you\",\"how'll\": \"how will\",\"how's\": \"how is\",\"I'd\": \"I would\",\"I'd've\": \"I would have\",\"I'll\": \"I will\",\"I'll've\": \"I will have\",\"I'm\": \"I am\",\"I've\": \"I have\",\n  \"isn't\": \"is not\",\"it'd\": \"it had\",\"it'd've\": \"it would have\",\"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\",\"let's\": \"let us\",\"ma'am\": \"madam\",\"mayn't\": \"may not\",\n  \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\",\"must've\": \"must have\",\"mustn't\": \"must not\",\"mustn't've\": \"must not have\",\"needn't\": \"need not\",\"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\"oughtn't\": \"ought not\",\"oughtn't've\": \"ought not have\",\"shan't\": \"shall not\",\"sha'n't\": \"shall not\",\n  \"shan't've\": \"shall not have\",\"she'd\": \"she would\",\"she'd've\": \"she would have\",\"she'll\": \"she will\",\"she'll've\": \"she will have\",\"she's\": \"she is\",\n  \"should've\": \"should have\",\"shouldn't\": \"should not\",\"shouldn't've\": \"should not have\",\"so've\": \"so have\",\"so's\": \"so is\",\"that'd\": \"that would\",\"that'd've\": \"that would have\",\"that's\": \"that is\",\"there'd\": \"there had\",\"there'd've\": \"there would have\",\"there's\": \"there is\",\"they'd\": \"they would\",\"they'd've\": \"they would have\",\"they'll\": \"they will\",\"they'll've\": \"they will have\",\"they're\": \"they are\",\"they've\": \"they have\",\"to've\": \"to have\",\"wasn't\": \"was not\",\"we'd\": \"we had\",\n  \"we'd've\": \"we would have\",\"we'll\": \"we will\",\"we'll've\": \"we will have\",\"we're\": \"we are\",\"we've\": \"we have\",\n  \"weren't\": \"were not\",\"what'll\": \"what will\",\"what'll've\": \"what will have\",\n  \"what're\": \"what are\",\"what's\": \"what is\",\"what've\": \"what have\",\"when's\": \"when is\",\"when've\": \"when have\",\n  \"where'd\": \"where did\",\"where's\": \"where is\",\"where've\": \"where have\",\"who'll\": \"who will\",\"who'll've\": \"who will have\",\"who's\": \"who is\",\"who've\": \"who have\",\"why's\": \"why is\",\n  \"why've\": \"why have\",\"will've\": \"will have\",\"won't\": \"will not\",\"won't've\": \"will not have\",\"would've\": \"would have\",\"wouldn't\": \"would not\",\n  \"wouldn't've\": \"would not have\",\"y'all\": \"you all\",\"y'alls\": \"you alls\",\"y'all'd\": \"you all would\",\n  \"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\"you'd\": \"you had\",\"you'd've\": \"you would have\",\"you'll\": \"you you will\",\"you'll've\": \"you you will have\",\"you're\": \"you are\",  \"you've\": \"you have\"\n   }\n\nc_re = re.compile('(%s)' % '|'.join(cList.keys()))\n\ndef expandContractions(text, c_re=c_re):\n    def replace(match):\n        return cList[match.group(0)]\n    return c_re.sub(replace, text)\n\ndef removeHTML(x):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',x)\ndef dataPreprocessing(x):\n    x = x.lower()\n    x = removeHTML(x)\n    x = re.sub(\"@\\w+\", '',x)\n    x = re.sub(\"'\\d+\", '',x)\n    x = re.sub(\"\\d+\", '',x)\n    x = re.sub(\"http\\w+\", '',x)\n    x = re.sub(r\"\\s+\", \" \", x)\n    x = re.sub(r\"\\.+\", \".\", x)\n    x = re.sub(r\"\\,+\", \",\", x)\n    x = x.strip()\n    return x","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:58.992166Z","iopub.status.idle":"2024-05-06T19:43:58.992543Z","shell.execute_reply.started":"2024-05-06T19:43:58.992353Z","shell.execute_reply":"2024-05-06T19:43:58.992368Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def remove_punctuation(text):\n    translator = str.maketrans('', '', string.punctuation)\n    return text.translate(translator)\n\ntext = \"Hello, world! This is a test.\"\nprint(remove_punctuation(text))","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:58.993842Z","iopub.status.idle":"2024-05-06T19:43:58.994216Z","shell.execute_reply.started":"2024-05-06T19:43:58.994037Z","shell.execute_reply":"2024-05-06T19:43:58.994052Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# paragraph features\ndef Paragraph_Preprocess(tmp):\n    # Expand the paragraph list into several lines of data\n    tmp = tmp.explode('paragraph')\n    # Paragraph preprocessing\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(dataPreprocessing))\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(remove_punctuation).alias('paragraph_no_pinctuation'))\n    tmp = tmp.with_columns(pl.col('paragraph_no_pinctuation').map_elements(count_spelling_errors).alias(\"paragraph_error_num\"))\n    # Calculate the length of each paragraph\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x)).alias(\"paragraph_len\"))\n    # Calculate the number of sentences and words in each paragraph\n    tmp = tmp.with_columns(pl.col('paragraph').map_elements(lambda x: len(x.split('.'))).alias(\"paragraph_sentence_cnt\"),\n                    pl.col('paragraph').map_elements(lambda x: len(x.split(' '))).alias(\"paragraph_word_cnt\"),)\n    return tmp\n\n# feature_eng\nparagraph_fea = ['paragraph_len','paragraph_sentence_cnt','paragraph_word_cnt']\nparagraph_fea2 = ['paragraph_error_num'] + paragraph_fea\ndef Paragraph_Eng(train_tmp):\n    num_list = [0, 50,75,100,125,150,175,200,250,300,350,400,500,600]\n    num_list2 = [0, 50,75,100,125,150,175,200,250,300,350,400,500,600,700]\n    aggs = [\n        # Count the number of paragraph lengths greater than and less than the i-value\n        *[pl.col('paragraph').filter(pl.col('paragraph_len') >= i).count().alias(f\"paragraph_>{i}_cnt\") for i in [0, 50,100,150,200,250,300,400,500,600,700] ], \n        *[pl.col('paragraph').filter(pl.col('paragraph_len') <= i).count().alias(f\"paragraph_<{i}_cnt\") for i in [25,49]], \n        # other\n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in paragraph_fea2],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in paragraph_fea2],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in paragraph_fea2],\n        *[pl.col(fea).sum().alias(f\"{fea}_sum\") for fea in paragraph_fea2],\n        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in paragraph_fea2],\n        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in paragraph_fea2],\n        *[pl.col(fea).kurtosis().alias(f\"{fea}_kurtosis\") for fea in paragraph_fea2],\n        *[pl.col(fea).quantile(0.25).alias(f\"{fea}_q1\") for fea in paragraph_fea2],  \n        *[pl.col(fea).quantile(0.75).alias(f\"{fea}_q3\") for fea in paragraph_fea2],  \n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\ntmp = Paragraph_Preprocess(train)\ntrain_feats = Paragraph_Eng(tmp)\ntrain_feats['score'] = train['score']\n\n# Obtain feature names\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:58.996416Z","iopub.status.idle":"2024-05-06T19:43:58.996793Z","shell.execute_reply.started":"2024-05-06T19:43:58.996590Z","shell.execute_reply":"2024-05-06T19:43:58.996604Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Генерация фичей и препроцессинг","metadata":{}},{"cell_type":"code","source":"# sentence feature\ndef Sentence_Preprocess(tmp):\n    # Preprocess full_text and use periods to segment sentences in the text\n    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\".\").alias(\"sentence\"))\n    tmp = tmp.explode('sentence')\n    # Calculate the length of a sentence\n    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x)).alias(\"sentence_len\"))\n    tmp = tmp.with_columns(pl.col('sentence').map_elements(lambda x: len(x.split(' '))).alias(\"sentence_word_cnt\"))\n    \n    return tmp\n# feature_eng\nsentence_fea = ['sentence_len','sentence_word_cnt']\ndef Sentence_Eng(train_tmp):\n    aggs = [\n        # Count the number of sentences with a length greater than i\n        *[pl.col('sentence').filter(pl.col('sentence_len') >= i).count().alias(f\"sentence_>{i}_cnt\") for i in [0,15,50,100,150,200,300] ], \n        *[pl.col('sentence').filter(pl.col('sentence_len') <= i).count().alias(f\"sentence_<{i}_cnt\") for i in [15,50] ], \n        # other\n        *[pl.col(fea).max().alias(f\"{fea}_max\") for fea in sentence_fea],\n        *[pl.col(fea).mean().alias(f\"{fea}_mean\") for fea in sentence_fea],\n        *[pl.col(fea).min().alias(f\"{fea}_min\") for fea in sentence_fea],\n        *[pl.col(fea).sum().alias(f\"{fea}_sum\") for fea in sentence_fea],\n        *[pl.col(fea).first().alias(f\"{fea}_first\") for fea in sentence_fea],\n        *[pl.col(fea).last().alias(f\"{fea}_last\") for fea in sentence_fea],\n        *[pl.col(fea).kurtosis().alias(f\"{fea}_kurtosis\") for fea in sentence_fea],\n        *[pl.col(fea).quantile(0.25).alias(f\"{fea}_q1\") for fea in sentence_fea],  \n        *[pl.col(fea).quantile(0.75).alias(f\"{fea}_q3\") for fea in sentence_fea],  \n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\ntmp = Sentence_Preprocess(train)\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:58.998164Z","iopub.status.idle":"2024-05-06T19:43:58.998500Z","shell.execute_reply.started":"2024-05-06T19:43:58.998333Z","shell.execute_reply":"2024-05-06T19:43:58.998346Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# word feature\ndef Word_Preprocess(tmp):\n    # Preprocess full_text and use spaces to separate words from the text\n    tmp = tmp.with_columns(pl.col('full_text').map_elements(dataPreprocessing).str.split(by=\" \").alias(\"word\"))\n    tmp = tmp.explode('word')\n    # Calculate the length of each word\n    tmp = tmp.with_columns(pl.col('word').map_elements(lambda x: len(x)).alias(\"word_len\"))\n    # Delete data with a word length of 0\n    tmp = tmp.filter(pl.col('word_len')!=0)\n    \n    return tmp\n# feature_eng\ndef Word_Eng(train_tmp):\n    aggs = [\n        # Count the number of words with a length greater than i+1\n        *[pl.col('word').filter(pl.col('word_len') >= i+1).count().alias(f\"word_{i+1}_cnt\") for i in range(15) ], \n        # other\n        pl.col('word_len').max().alias(f\"word_len_max\"),\n        pl.col('word_len').mean().alias(f\"word_len_mean\"),\n        pl.col('word_len').std().alias(f\"word_len_std\"),\n        pl.col('word_len').quantile(0.25).alias(f\"word_len_q1\"),\n        pl.col('word_len').quantile(0.50).alias(f\"word_len_q2\"),\n        pl.col('word_len').quantile(0.75).alias(f\"word_len_q3\"),\n        ]\n    df = train_tmp.group_by(['essay_id'], maintain_order=True).agg(aggs).sort(\"essay_id\")\n    df = df.to_pandas()\n    return df\n\ntmp = Word_Preprocess(train)\n# Merge the newly generated feature data with the previously generated feature data\ntrain_feats = train_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))\ntrain_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:58.999575Z","iopub.status.idle":"2024-05-06T19:43:58.999965Z","shell.execute_reply.started":"2024-05-06T19:43:58.999785Z","shell.execute_reply":"2024-05-06T19:43:58.999800Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(\n            tokenizer=lambda x: x,\n            preprocessor=lambda x: x,\n            token_pattern=None,\n            strip_accents='unicode',\n            analyzer = 'word',\n            ngram_range=(3,6),\n            min_df=0.05,\n            max_df=0.95,\n            sublinear_tf=True,\n)\n\ntrain_tfid = vectorizer.fit_transform([i for i in train['full_text']])\ndense_matrix = train_tfid.toarray()\ndf = pd.DataFrame(dense_matrix)\ntfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = train_feats['essay_id']\ntrain_feats = train_feats.merge(df, on='essay_id', how='left')\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Number of Features: ',len(feature_names))\ntrain_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:59.001168Z","iopub.status.idle":"2024-05-06T19:43:59.001540Z","shell.execute_reply.started":"2024-05-06T19:43:59.001351Z","shell.execute_reply":"2024-05-06T19:43:59.001366Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"vectorizer_cnt = CountVectorizer(\n            tokenizer=lambda x: x,\n            preprocessor=lambda x: x,\n            token_pattern=None,\n            strip_accents='unicode',\n            analyzer = 'word',\n            ngram_range=(2,3),\n            min_df=0.10,\n            max_df=0.85,\n)\ntrain_tfid = vectorizer_cnt.fit_transform([i for i in train['full_text']])\ndense_matrix = train_tfid.toarray()\ndf = pd.DataFrame(dense_matrix)\ntfid_columns = [ f'tfid_cnt_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = train_feats['essay_id']\ntrain_feats = train_feats.merge(df, on='essay_id', how='left')","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:59.003176Z","iopub.status.idle":"2024-05-06T19:43:59.003607Z","shell.execute_reply.started":"2024-05-06T19:43:59.003384Z","shell.execute_reply":"2024-05-06T19:43:59.003401Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Тренировка модели","metadata":{}},{"cell_type":"code","source":"import joblib\n\ndeberta_oof = joblib.load('/kaggle/input/aes2-400-20240419134941/oof.pkl')\nprint(deberta_oof.shape, train_feats.shape)\n\nfor i in range(6):\n    train_feats[f'deberta_oof_{i}'] = deberta_oof[:, i]\n\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], train_feats.columns))\nprint('Features Number: ',len(feature_names))    \n\ntrain_feats.shape","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:59.004975Z","iopub.status.idle":"2024-05-06T19:43:59.005421Z","shell.execute_reply.started":"2024-05-06T19:43:59.005203Z","shell.execute_reply":"2024-05-06T19:43:59.005220Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def quadratic_weighted_kappa(y_true, y_pred):\n    y_true = y_true + a\n    y_pred = (y_pred + a).clip(1, 6).round()\n    qwk = cohen_kappa_score(y_true, y_pred, weights=\"quadratic\")\n    return 'QWK', qwk, True\ndef qwk_obj(y_true, y_pred):\n    labels = y_true + a\n    preds = y_pred + a\n    preds = preds.clip(1, 6)\n    f = 1/2*np.sum((preds-labels)**2)\n    g = 1/2*np.sum((preds-a)**2+b)\n    df = preds - labels\n    dg = preds - a\n    grad = (df/g - f*dg/g**2)*len(labels)\n    hess = np.ones(len(labels))\n    return grad, hess\na = 2.998\nb = 1.092","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:59.006924Z","iopub.status.idle":"2024-05-06T19:43:59.007362Z","shell.execute_reply.started":"2024-05-06T19:43:59.007142Z","shell.execute_reply":"2024-05-06T19:43:59.007160Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = train_feats[feature_names].astype(np.float32).values\n\ny_split = train_feats['score'].astype(int).values\ny = train_feats['score'].astype(np.float32).values-a\noof = train_feats['score'].astype(int).values","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:59.008932Z","iopub.status.idle":"2024-05-06T19:43:59.009312Z","shell.execute_reply.started":"2024-05-06T19:43:59.009132Z","shell.execute_reply":"2024-05-06T19:43:59.009146Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"len(feature_names)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:59.010706Z","iopub.status.idle":"2024-05-06T19:43:59.011037Z","shell.execute_reply.started":"2024-05-06T19:43:59.010878Z","shell.execute_reply":"2024-05-06T19:43:59.010891Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"def feature_select_wrapper():   \n    print('feature_select_wrapper...')\n    features = feature_names\n    skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n    fse = pd.Series(0, index=features)  \n    for train_index, test_index in skf.split(X, y_split):\n        X_train_fold, X_test_fold = X[train_index], X[test_index]\n        y_train_fold, y_test_fold, y_test_fold_int = y[train_index], y[test_index], y_split[test_index]\n        model = lgb.LGBMRegressor(\n                    objective = qwk_obj,\n                    metrics = 'None',\n                    learning_rate = 0.05,\n                    max_depth = 5,\n                    num_leaves = 10,\n                    colsample_bytree=0.3,\n                    reg_alpha = 0.7,\n                    reg_lambda = 0.1,\n                    n_estimators=700,\n                    random_state=412,\n                    extra_trees=True,\n                    class_weight='balanced',\n                    verbosity = - 1)\n\n        predictor = model.fit(X_train_fold,\n                                      y_train_fold,\n                                      eval_names=['train', 'valid'],\n                                      eval_set=[(X_train_fold, y_train_fold), (X_test_fold, y_test_fold)],\n                                      eval_metric=quadratic_weighted_kappa,\n                                      callbacks=callbacks,)\n        models.append(predictor)\n        predictions_fold = predictor.predict(X_test_fold)\n        predictions_fold = predictions_fold + a\n        oof[test_index]=predictions_fold\n        predictions_fold = predictions_fold.clip(1, 6).round()\n        predictions.append(predictions_fold)\n        f1_fold = f1_score(y_test_fold_int, predictions_fold, average='weighted')\n        f1_scores.append(f1_fold)\n\n\n        kappa_fold = cohen_kappa_score(y_test_fold_int, predictions_fold, weights='quadratic')\n        kappa_scores.append(kappa_fold)\n\n        cm = confusion_matrix(y_test_fold_int, predictions_fold, labels=[x for x in range(1,7)])\n\n        disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                                      display_labels=[x for x in range(1,7)])\n        disp.plot()\n        plt.show()\n        print(f'F1 score across fold: {f1_fold}')\n        print(f'Cohen kappa score across fold: {kappa_fold}')\n\n        fse += pd.Series(predictor.feature_importances_, features)\n    \n    feature_select = fse.sort_values(ascending=False).index.tolist()[:13000]\n    print('done')\n    return feature_select","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:59.012742Z","iopub.status.idle":"2024-05-06T19:43:59.013199Z","shell.execute_reply.started":"2024-05-06T19:43:59.012978Z","shell.execute_reply":"2024-05-06T19:43:59.012995Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"f1_scores = []\nkappa_scores = []\nmodels = []\npredictions = []\ncallbacks = [log_evaluation(period=25), early_stopping(stopping_rounds=75,first_metric_only=True)]\nfeature_select = feature_select_wrapper()","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:59.015117Z","iopub.status.idle":"2024-05-06T19:43:59.015544Z","shell.execute_reply.started":"2024-05-06T19:43:59.015324Z","shell.execute_reply":"2024-05-06T19:43:59.015341Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X = train_feats[feature_select].astype(np.float32).values","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:59.017019Z","iopub.status.idle":"2024-05-06T19:43:59.017467Z","shell.execute_reply.started":"2024-05-06T19:43:59.017228Z","shell.execute_reply":"2024-05-06T19:43:59.017246Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Кросс-Валидация","metadata":{}},{"cell_type":"code","source":"n_splits = 15\n\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=0)\n\nf1_scores = []\nkappa_scores = []\nmodels = []\npredictions = []\ncallbacks = [log_evaluation(period=25), early_stopping(stopping_rounds=75,first_metric_only=True)]\n\ni=1\nfor train_index, test_index in skf.split(X, y_split):\n   \n    print('fold',i)\n    X_train_fold, X_test_fold = X[train_index], X[test_index]\n    \n   \n    y_train_fold, y_test_fold, y_test_fold_int = y[train_index], y[test_index], y_split[test_index]\n    \n    model = lgb.LGBMRegressor(\n                objective = qwk_obj,\n                metrics = 'None',\n                learning_rate = 0.05,\n                max_depth = 5,\n                num_leaves = 10,\n                colsample_bytree=0.3,\n                reg_alpha = 0.7,\n                reg_lambda = 0.1,\n                n_estimators=700,\n                random_state=42,\n                extra_trees=True,\n                class_weight='balanced',\n                verbosity = - 1)\n\n    predictor = model.fit(X_train_fold,\n                                  y_train_fold,\n                                  eval_names=['train', 'valid'],\n                                  eval_set=[(X_train_fold, y_train_fold), (X_test_fold, y_test_fold)],\n                                  eval_metric=quadratic_weighted_kappa,\n                                  callbacks=callbacks,)\n    models.append(predictor)\n    predictions_fold = predictor.predict(X_test_fold)\n    predictions_fold = predictions_fold + a\n    oof[test_index]=predictions_fold\n    predictions_fold = predictions_fold.clip(1, 6).round()\n    predictions.append(predictions_fold)\n    f1_fold = f1_score(y_test_fold_int, predictions_fold, average='weighted')\n    f1_scores.append(f1_fold)\n    \n    \n    kappa_fold = cohen_kappa_score(y_test_fold_int, predictions_fold, weights='quadratic')\n    kappa_scores.append(kappa_fold)\n    \n    cm = confusion_matrix(y_test_fold_int, predictions_fold, labels=[x for x in range(1,7)])\n\n    disp = ConfusionMatrixDisplay(confusion_matrix=cm,\n                                  display_labels=[x for x in range(1,7)])\n    disp.plot()\n    plt.show()\n    print(f'F1 score across fold: {f1_fold}')\n    print(f'Cohen kappa score across fold: {kappa_fold}')\n    i+=1\n\nmean_f1_score = np.mean(f1_scores)\nmean_kappa_score = np.mean(kappa_scores)\n\nprint(f'Mean F1 score across {n_splits} folds: {mean_f1_score}')\nprint(f'Mean Cohen kappa score across {n_splits} folds: {mean_kappa_score}')","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:59.018643Z","iopub.status.idle":"2024-05-06T19:43:59.019176Z","shell.execute_reply.started":"2024-05-06T19:43:59.018904Z","shell.execute_reply":"2024-05-06T19:43:59.018925Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open('models.pkl', 'wb') as f:\n    pickle.dump(models, f)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:59.020893Z","iopub.status.idle":"2024-05-06T19:43:59.021354Z","shell.execute_reply.started":"2024-05-06T19:43:59.021124Z","shell.execute_reply":"2024-05-06T19:43:59.021143Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with open('models.pkl', 'rb') as f:\n    models = pickle.load(f)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:59.022833Z","iopub.status.idle":"2024-05-06T19:43:59.023287Z","shell.execute_reply.started":"2024-05-06T19:43:59.023057Z","shell.execute_reply":"2024-05-06T19:43:59.023075Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"### Инференс модели","metadata":{}},{"cell_type":"code","source":"# Paragraph\ntmp = Paragraph_Preprocess(test)\ntest_feats = Paragraph_Eng(tmp)\n# Sentence\ntmp = Sentence_Preprocess(test)\ntest_feats = test_feats.merge(Sentence_Eng(tmp), on='essay_id', how='left')\n# Word\ntmp = Word_Preprocess(test)\ntest_feats = test_feats.merge(Word_Eng(tmp), on='essay_id', how='left')\n\n# Tfidf\ntest_tfid = vectorizer.transform([i for i in test['full_text']])\ndense_matrix = test_tfid.toarray()\ndf = pd.DataFrame(dense_matrix)\ntfid_columns = [ f'tfid_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = test_feats['essay_id']\ntest_feats = test_feats.merge(df, on='essay_id', how='left')\n\n# CountVectorizer\ntest_tfid = vectorizer_cnt.transform([i for i in test['full_text']])\ndense_matrix = test_tfid.toarray()\ndf = pd.DataFrame(dense_matrix)\ntfid_columns = [ f'tfid_cnt_{i}' for i in range(len(df.columns))]\ndf.columns = tfid_columns\ndf['essay_id'] = test_feats['essay_id']\ntest_feats = test_feats.merge(df, on='essay_id', how='left')\n\nfor i in range(6):\n    test_feats[f'deberta_oof_{i}'] = predicted_score[:, i]\n\n# Features number\nfeature_names = list(filter(lambda x: x not in ['essay_id','score'], test_feats.columns))\nprint('Features number: ',len(feature_names))\ntest_feats.head(3)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:59.025185Z","iopub.status.idle":"2024-05-06T19:43:59.025642Z","shell.execute_reply.started":"2024-05-06T19:43:59.025408Z","shell.execute_reply":"2024-05-06T19:43:59.025427Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"probabilities = []\nfor model in models:\n    proba= model.predict(test_feats[feature_select])+ a\n    probabilities.append(proba)\n\npredictions = np.mean(probabilities, axis=0)\n\npredictions = np.round(predictions.clip(1, 6))\n\nprint(predictions)","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:59.026964Z","iopub.status.idle":"2024-05-06T19:43:59.027296Z","shell.execute_reply.started":"2024-05-06T19:43:59.027136Z","shell.execute_reply":"2024-05-06T19:43:59.027149Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"submission=pd.read_csv(\"/kaggle/input/learning-agency-lab-automated-essay-scoring-2/sample_submission.csv\")\nsubmission['score']=predictions\nsubmission['score']=submission['score'].astype(int)\nsubmission.to_csv(\"submission.csv\",index=None)\ndisplay(submission.head())","metadata":{"execution":{"iopub.status.busy":"2024-05-06T19:43:59.028570Z","iopub.status.idle":"2024-05-06T19:43:59.028923Z","shell.execute_reply.started":"2024-05-06T19:43:59.028763Z","shell.execute_reply":"2024-05-06T19:43:59.028777Z"},"trusted":true},"outputs":[],"execution_count":null}]}